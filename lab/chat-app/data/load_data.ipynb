{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "### TOC\n",
    "- [0️⃣ Initialize notebook variables](#0)\n",
    "- [1️⃣ Create an Azure AI Search index and load data](#1)\n",
    "- [2️⃣ Create a Postgres database and load data](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0️⃣ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management) \n",
    "- Adjust the OpenAI model and version according the [availability by region.](https://learn.microsoft.com/azure/ai-services/openai/concepts/models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from faker import Faker\n",
    "\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from azure.identity import AzureDeveloperCliCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    AzureOpenAIParameters,\n",
    "    AzureOpenAIVectorizer,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    IndexProjectionMode,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerDataSourceType,\n",
    "    SearchIndexerIndexProjections,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SimpleField,\n",
    "    SplitSkill,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "load_dotenv(\"../python/.env\", override=True)\n",
    "\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "azure_search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "azure_storage_connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "azure_storage_container = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "azure_openai_embedding_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\")\n",
    "azure_openai_embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\")\n",
    "azure_openai_embeddings_dimensions = 3072\n",
    "indexer_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "azure_storage_endpoint = f\"https://{os.getenv('AZURE_STORAGE_NAME')}.blob.core.windows.net\"\n",
    "\n",
    "\n",
    "data_file = \"./comisiones.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 1️⃣ Load data to Azure Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "        def info(self, msg, *args):\n",
    "            if args:\n",
    "                print(msg % args)\n",
    "            else:\n",
    "                print(msg)\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "def setup_index(azure_credential, index_name, azure_search_endpoint, azure_storage_connection_string, azure_storage_container, azure_openai_embedding_endpoint, azure_openai_embedding_deployment, azure_openai_embedding_model, azure_openai_embeddings_dimensions):\n",
    "    index_client = SearchIndexClient(azure_search_endpoint, azure_credential)\n",
    "    indexer_client = SearchIndexerClient(azure_search_endpoint, azure_credential)\n",
    "\n",
    "    data_source_connections = indexer_client.get_data_source_connections()\n",
    "    if index_name in [ds.name for ds in data_source_connections]:\n",
    "        logger.info(f\"Data source connection {index_name} already exists, not re-creating\")\n",
    "    else:\n",
    "        logger.info(f\"Creating data source connection: {index_name}\")\n",
    "        indexer_client.create_data_source_connection(\n",
    "            data_source_connection=SearchIndexerDataSourceConnection(\n",
    "                name=index_name, \n",
    "                type=SearchIndexerDataSourceType.AZURE_BLOB,\n",
    "                connection_string=azure_storage_connection_string,\n",
    "                container=SearchIndexerDataContainer(name=azure_storage_container)))\n",
    "\n",
    "    index_names = [index.name for index in index_client.list_indexes()]\n",
    "    if index_name in index_names:\n",
    "        logger.info(f\"Index {index_name} already exists, not re-creating\")\n",
    "    else:\n",
    "        logger.info(f\"Creating index: {index_name}\")\n",
    "        index_client.create_index(\n",
    "            SearchIndex(\n",
    "                name=index_name,\n",
    "                fields=[\n",
    "                    SearchableField(name=\"chunk_id\", key=True, analyzer_name=\"keyword\", sortable=True),\n",
    "                    SimpleField(name=\"parent_id\", type=SearchFieldDataType.String, filterable=True),\n",
    "                    SearchableField(name=\"title\"),\n",
    "                    SearchableField(name=\"chunk\"),\n",
    "                    SearchField(\n",
    "                        name=\"text_vector\", \n",
    "                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                        vector_search_dimensions=EMBEDDINGS_DIMENSIONS,\n",
    "                        vector_search_profile_name=\"vp\",\n",
    "                        stored=True,\n",
    "                        hidden=False)\n",
    "                ],\n",
    "                vector_search=VectorSearch(\n",
    "                    algorithms=[\n",
    "                        HnswAlgorithmConfiguration(name=\"algo\", parameters=HnswParameters(metric=VectorSearchAlgorithmMetric.COSINE))\n",
    "                    ],\n",
    "                    vectorizers=[\n",
    "                        AzureOpenAIVectorizer(\n",
    "                            name=\"openai_vectorizer\",\n",
    "                            azure_open_ai_parameters=AzureOpenAIParameters(\n",
    "                                resource_uri=azure_openai_embedding_endpoint,\n",
    "                                deployment_id=azure_openai_embedding_deployment,\n",
    "                                model_name=azure_openai_embedding_model\n",
    "                            )\n",
    "                        )\n",
    "                    ],\n",
    "                    profiles=[\n",
    "                        VectorSearchProfile(name=\"vp\", algorithm_configuration_name=\"algo\", vectorizer=\"openai_vectorizer\")\n",
    "                    ]\n",
    "                ),\n",
    "                semantic_search=SemanticSearch(\n",
    "                    configurations=[\n",
    "                        SemanticConfiguration(\n",
    "                            name=\"default\",\n",
    "                            prioritized_fields=SemanticPrioritizedFields(title_field=SemanticField(field_name=\"title\"), content_fields=[SemanticField(field_name=\"chunk\")])\n",
    "                        )\n",
    "                    ],\n",
    "                    default_configuration_name=\"default\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    skillsets = indexer_client.get_skillsets()\n",
    "    if index_name in [skillset.name for skillset in skillsets]:\n",
    "        logger.info(f\"Skillset {index_name} already exists, not re-creating\")\n",
    "    else:\n",
    "        logger.info(f\"Creating skillset: {index_name}\")\n",
    "        indexer_client.create_skillset(\n",
    "            skillset=SearchIndexerSkillset(\n",
    "                name=index_name,\n",
    "                skills=[\n",
    "                    SplitSkill(\n",
    "                        text_split_mode=\"pages\",\n",
    "                        context=\"/document\",\n",
    "                        maximum_page_length=2000,\n",
    "                        page_overlap_length=500,\n",
    "                        inputs=[InputFieldMappingEntry(name=\"text\", source=\"/document/content\")],\n",
    "                        outputs=[OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")]),\n",
    "                    AzureOpenAIEmbeddingSkill(\n",
    "                        context=\"/document/pages/*\",\n",
    "                        resource_uri=azure_openai_embedding_endpoint,\n",
    "                        api_key=None,\n",
    "                        deployment_id=azure_openai_embedding_deployment,\n",
    "                        model_name=azure_openai_embedding_model,\n",
    "                        dimensions=azure_openai_embeddings_dimensions,\n",
    "                        inputs=[InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\")],\n",
    "                        outputs=[OutputFieldMappingEntry(name=\"embedding\", target_name=\"text_vector\")])\n",
    "                ],\n",
    "                index_projections=SearchIndexerIndexProjections(\n",
    "                    selectors=[\n",
    "                        SearchIndexerIndexProjectionSelector(\n",
    "                            target_index_name=index_name,\n",
    "                            parent_key_field_name=\"parent_id\",\n",
    "                            source_context=\"/document/pages/*\",\n",
    "                            mappings=[\n",
    "                                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),\n",
    "                                InputFieldMappingEntry(name=\"text_vector\", source=\"/document/pages/*/text_vector\"),\n",
    "                                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\")\n",
    "                            ]\n",
    "                        )\n",
    "                    ],\n",
    "                    parameters=SearchIndexerIndexProjectionsParameters(\n",
    "                        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS\n",
    "                    )\n",
    "                )))\n",
    "\n",
    "    indexers = indexer_client.get_indexers()\n",
    "    if index_name in [indexer.name for indexer in indexers]:\n",
    "        logger.info(f\"Indexer {index_name} already exists, not re-creating\")\n",
    "    else:\n",
    "        indexer_client.create_indexer(\n",
    "            indexer=SearchIndexer(\n",
    "                name=index_name,\n",
    "                data_source_name=index_name,\n",
    "                skillset_name=index_name,\n",
    "                target_index_name=index_name,        \n",
    "                field_mappings=[FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\")]\n",
    "            )\n",
    "        )\n",
    "\n",
    "def upload_documents(azure_credential, indexer_name, azure_search_endpoint, azure_storage_endpoint, azure_storage_container):\n",
    "    indexer_client = SearchIndexerClient(azure_search_endpoint, azure_credential)\n",
    "    # Upload the documents in /data folder to the blob storage container\n",
    "    blob_client = BlobServiceClient(\n",
    "        account_url=azure_storage_endpoint, credential=azure_credential,\n",
    "        max_single_put_size=4 * 1024 * 1024\n",
    "    )\n",
    "    container_client = blob_client.get_container_client(azure_storage_container)\n",
    "    if not container_client.exists():\n",
    "        container_client.create_container()\n",
    "    existing_blobs = [blob.name for blob in container_client.list_blobs()]\n",
    "\n",
    "    # Open each file in /data folder\n",
    "    for file in os.scandir(\"data\"):\n",
    "        with open(file.path, \"rb\") as opened_file:\n",
    "            filename = os.path.basename(file.path)\n",
    "            # Check if blob already exists\n",
    "            if filename in existing_blobs:\n",
    "                logger.info(\"Blob already exists, skipping file: %s\", filename)\n",
    "            else:\n",
    "                logger.info(\"Uploading blob for file: %s\", filename)\n",
    "                blob_client = container_client.upload_blob(filename, opened_file, overwrite=True)\n",
    "\n",
    "    # Start the indexer\n",
    "    try:\n",
    "        indexer_client.run_indexer(indexer_name)\n",
    "        logger.info(\"Indexer started. Any unindexed blobs should be indexed in a few minutes, check the Azure Portal for status.\")\n",
    "    except ResourceExistsError:\n",
    "        logger.info(\"Indexer already running, not starting again\")\n",
    "\n",
    "\n",
    "azure_credential = AzureDeveloperCliCredential(tenant_id=os.environ[\"AZURE_TENANT_ID\"], process_timeout=60)\n",
    "\n",
    "setup_index(azure_credential,\n",
    "    index_name, \n",
    "    azure_search_endpoint,\n",
    "    azure_storage_connection_string,\n",
    "    azure_storage_container,\n",
    "    azure_openai_embedding_endpoint,\n",
    "    azure_openai_embedding_deployment,\n",
    "    azure_openai_embedding_model,\n",
    "    azure_openai_embeddings_dimensions)\n",
    "\n",
    "upload_documents(azure_credential,\n",
    "    indexer_name,\n",
    "    azure_search_endpoint,\n",
    "    azure_storage_endpoint,\n",
    "    azure_storage_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2️⃣ Load data to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Database connection parameters from environment variables\n",
    "db_params = {\n",
    "    \"database\": os.getenv(\"AZURE_POSTGRES_DATABASE\"),\n",
    "    \"user\": f\"{os.getenv('AZURE_POSTGRES_USER')}\",\n",
    "    \"password\": os.getenv(\"AZURE_POSTGRES_PASSWORD\"),\n",
    "    \"host\": f\"{os.getenv('AZURE_POSTGRES_SERVER')}\", \n",
    "    \"port\": 5432,\n",
    "    \"sslmode\": \"require\"\n",
    "}\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create the detallespoliza table if it doesn't exist\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bank_transactions (\n",
    "    transaction_id SERIAL PRIMARY KEY,\n",
    "    account_number VARCHAR(20),\n",
    "    customer_name VARCHAR(255),\n",
    "    transaction_date DATE,\n",
    "    transaction_type VARCHAR(10),\n",
    "    amount DECIMAL(10,2),\n",
    "    description TEXT\n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO bank_transactions (\n",
    "    account_number, customer_name, transaction_date, transaction_type, amount, description\n",
    ") VALUES (\n",
    "    %s, %s, %s, %s, %s, %s\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "for _ in range(1000):\n",
    "    tran_type = fake.random_element(elements=[\"debit\", \"credit\"])\n",
    "    amt = round(random.uniform(10, 5000), 2)\n",
    "    record = (\n",
    "        fake.bban(),  # account_numberer\n",
    "        fake.name(),  # customer_name),  # customer_name\n",
    "        fake.date_between(start_date=\"-5y\", end_date=\"today\"),  # transaction_dateart_date=\"-5y\", end_date=\"today\"),  # transaction_date\n",
    "        tran_type,  # transaction_type\n",
    "        amt,  # amount\n",
    "        fake.sentence()  # description\n",
    "    )\n",
    "    cur.execute(insert_query, record)\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
